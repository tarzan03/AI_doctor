# AI_doctor
Smart medical chatbot â€“ multimodal input, voice-enabled output
ðŸ©º Medical AI Chatbot

An intelligent multimodal medical chatbot powered by LLaMA 17B Instruct, designed to handle both text + image inputs and respond with natural voice answers.

ðŸ”¹ Key Features

ðŸ§  LLaMA 17B Instruct â€“ robust instruction-tuned model for medical Q&A

ðŸ“¸ Multimodal Input â€“ supports text and image queries for flexible interaction

ðŸ”Š Voice Output with ElevenLabs â€“ generates natural, human-like speech for answers

ðŸ–¥ Gradio Frontend â€“ clean, interactive, and user-friendly UI

âš¡ FastAPI Backend â€“ scalable and high-performance API layer

ðŸŽ¯ What it does

Accepts medical queries via text or images (e.g., prescriptions, medical scans, symptoms)

Provides contextual, instruction-based answers using LLaMA

Converts responses into realistic voice outputs for a more natural interaction

Serves as a prototype for AI-assisted healthcare communication

ðŸ”§ Tech Stack

Model: LLaMA 17B Instruct (multimodal)

Backend: FastAPI

Frontend: Gradio

Voice Engine: ElevenLabs

ðŸŒŸ Why this project?

This project is a personal exploration into multimodal AI for healthcare â€“ combining vision, language, and speech into a single assistant. It demonstrates how LLMs + multimodal inputs + voice synthesis can be integrated to create more accessible and interactive medical chatbots.
